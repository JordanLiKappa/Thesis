Things to be included:
FTAG chpater:
0. clarify fixed cut
0. Scale factos are repeated in the calibration method and result section. Remove the repeated part
1.IP2D/3D definition in object reco definition
2.remove the overlapping samples with the Data/MC section and add extra samples. 
check if modulo ttV is an extra sample specific to the calibration
3. PFlow and VR track jet definition in object reco definition
4. refer to the experimental uncertaity in analysis 

Detector chapeter:
1. add reference to "more details in the parton model section"
2. check definition of pileup

Reconstruction chapter:
1. remove truth information if mention earlier
Kalman filter:
in its linear form the Kalman filter is the optimal recursive estimator of the state vector of a (discrete) linear dynamic system.
In such a system the evolution of the state vector is described by a linear transformation plus a random disturbance w, 
which is the process noise:
x_k = F_(k-1)x_(k-1) + w_(k-1),
Gaussian-sum flter GSF: 
The GSF method [32] is based on a generalisation of
the Kalman filter [33] and takes into account the non-linear
effects related to bremsstrahlung. Within the GSF, experi-
mental noise is modelled by a sum of Gaussian functions.
The GSF therefore consists of a number of Kalman filters
running in parallel, the result of which is that each track
parameter is approximated by a weighted sum of Gaussian
functions.where F is the propagator from detector k-1 to k

2. muon is minimaly ionising. For particles with mass much bigger than electron, 
the energy is lost mostly by ionisation. The rate of lost is a function of beta*gamma. 
Beyond the maximum, stopping power decreases approximately like 1/v2 with increasing particle velocity v, 
but after a minimum, it increases again (bremsstrahlung starting to become important).
The minimum for mu interaction with cupper is around 5 GeV, and after 1 TeV the bremsstrahlung starts to
take off. 
  


2. TODO: Find electron identification and isolation working points for bbtautau and Ftag

4.  TODO: link the RNN 0.01 to the fake factor section
5. think about why random tau selection is needed
6. check the rejection rate of the BDT for reject electron faking tau
Important points (from Nikos):  
5. check what muon is used for bbtautau
1. calibration for electron and muons?
8. why RNN tau is better than bdt? 
9. does anti-tau fit in the object reconstruction? 

DiHiggs:
1. how to explain non res signal is normalised to the SM cross section while the resonant cross section is arbitrary?
2. TODO: add explaination to MVA input legend 
5. TODO: consider remove the HEFT results
7. TODO: check each channel characteristics


DiHiggs note:
1. Fastsim: 
Almost 80% of the full simulation time is spent simu-
lating particles traversing the calorimetry, and about 75%
of the full simulation time is spent simulating electromag-
netic particles. Fast sim is to remove low energy electro magnetic 
particles from the calorimeter and replace them
with pre-simulated showers stored in memory
2. Regularization:
Regularization is a technique used for tuning the
 function by adding an additional penalty term in the error function.
  The additional term controls the excessively fluctuating function 
  such that the coefficients don’t take extreme values.
   This technique of keeping a check or reducing the value of 
   error coefficients are called shrinkage methods or weight decay in case of neural network
2.1
L2 regularization:
Ridge regression adds “squared magnitude” of coefficient 
as penalty term to the loss function. Here the highlighted part represents L2 regularization element.
L1: adds absolute value of magnitude


3.
fitting uncertainties:Because you are adding more information in the form of new data.  A couple of examples that might help understand:
- Imagine you have a electron E scale uncertainty measured early in run from a partial dataset. If you then do a search with the full data and have e.g. a ZCR with a Z->ee peak this provides new/more info on the electron energy scale so can reduce the uncertainty on that
- If you take for example a scale uncertainty from MC these come from general variations of the scale used in the generator but it might be that one of these variations doesn't describe the data you are measuring e.g. in a CR and hence the data can rule this out and reduce the uncertainty.   
- As a final example say you do something we often do any say my QCD background is small so I slap a 100% uncertainty on it.  When you fit, depending on which regions you include, it can potentially work out that the error is actually smaller since a 100% shift is not compatible with the data

Of course this comes with issues as well and you have to think about if you should be constraining the data.  Does it make sense.  For example, in the first case above if the electron E scale was already measured in the full dataset carefully by the e/gamma group it is unlikely your search  CR with the same amount of data would add info so if you heavily constrained that NP you might be concerned.  


4. renormalizastion scale and factorisation scale:
The cross section of hard scattering depends on matrix element square of |M(ij→kX)|2 (i,j,k are the spices of parton, 
X is the resulting hadron)
will lead to divergences, which come in two flavours:
   I the ultraviolet (UV) ones, which appear because of large momentum in the loops of the Feynman diagrams representing the amplitude;
   II the infrared (IR) ones, which appear because (i) either a virtual or a real particle can reach a zero momentum, 
   or because (ii) a massless particle radiates another massless particle.
The UV divergences are cured by introducing the renormalisation scale μR, 
of which the coupling constant αS becomes a function of. The IR divergences in case 
(i) cancel out (as predicted by the Kinoshita-Lee-Nauenberg theorem) but not in case (ii): 
they are cured by introducing the factorisation scale μF, of which the parton distribution and fragmentation functions will become a function of.
At this point, it is very important to understand that μF and μR are spurious parameters and that physical observable should ideally not depend on them. 
This would be true if we could sum the entire perturbation series, which is not only practically impossible but theoretically unsound. 
But at least, the more terms of the series we compute, the less the observable depends on those scales.

5. CKKW is alternative approach, aiming at an improved description of multi-jet topologies


DiHiggs todo:
1. check MMC, how it's done, is it described in eearlier chpater?
2. acc * eff uncertainties?
3. Prepare for questions in hyper parameter optimization
4. Prepare for question on Van Der Meer method and Lucid detector






DiHiggs question:
why the SLT sensivity dominates the LTT in high mass resonance, while it's comparable in the low mass?
ltt was assigned with low lepton pt threshold, and in high mass resonances slt has high efficiency and has priority 

why bbbb dominates the high energy region? 
can I still say the data is compatible with null hypothesis given we observed an excess?
Compatibility: what defines if two excess are compatible?

Theory question: 
why vaccuum state breaks the U(1) symmetry? A: it doesn't break the symmetry in lagrangian, but the state itself
is not invariant in U(1) transformation. 
Why vacuum state function has one 0 and one equals v:
you can try the algebra with non zero vev on the other one,you will see that electric charge would not be conserved
the vacuum itself would be electrically charged
why is the higgs mass correction proportional to squared of energy?
Is the generic spin 0 scalar limited to type one two three 2HDM?

Theory todo: 
1. TODO: add comments on bbWW channel
2. TODO: 2.9 Higgs boson pair production
I think in this section you need to say why HH production is important (i.e to resolve the potential and study EWSB), linking it to the previous chapters.  
The main message should be that while we have measure a lot about the Higgs we don't know if it really behaves like the SM mechanism with the wine-glass potential.
3.Somewhere (maybe after the big questions) you need to explicitly 
state that potential BSM enhancements to HH can be resonant or non-resonant 
(+ what that means).  This would then allow you to motivate the 2HDM and EFT as 
one example of each that are used as benchmark signals in the analysis.

Overall question:
Add a small section for ATLAS software?




